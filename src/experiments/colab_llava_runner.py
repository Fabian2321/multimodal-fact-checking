#!/usr/bin/env python3
"""
Google Colab LLaVA Runner for Text-Image Matching

This script runs the LLaVA-1.5-7B experiment on Google Colab with GPU acceleration.
It uses the exact same prompt and parameters as the local run.

Usage in Colab:
1. Upload this script to Colab
2. Upload the test_balanced_pairs_clean.csv file
3. Run the script
"""

import os
import pandas as pd
import torch
from transformers import LlavaForConditionalGeneration, LlavaProcessor
import requests
from PIL import Image
import io
import time
from typing import List, Dict, Any
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLaVAAnswerParser:
    """Parser for LLaVA model outputs to extract binary predictions"""
    
    def extract_prediction(self, generated_text: str) -> tuple[int, float, str]:
        """
        Extract binary prediction from LLaVA generated text.
        
        Args:
            generated_text: Raw text generated by LLaVA
            
        Returns:
            tuple: (predicted_label, confidence, explanation)
        """
        text = generated_text.lower().strip()
        
        # Look for explicit yes/no answers
        if any(word in text for word in ['yes', 'true', 'correct', 'accurate', 'matches']):
            return 1, 0.9, "Explicit positive response"
        elif any(word in text for word in ['no', 'false', 'incorrect', 'inaccurate', 'does not match']):
            return 0, 0.9, "Explicit negative response"
        
        # Look for confidence indicators
        if any(word in text for word in ['definitely', 'certainly', 'clearly']):
            confidence = 0.8
        elif any(word in text for word in ['probably', 'likely', 'seems']):
            confidence = 0.6
        else:
            confidence = 0.5
        
        # Default to negative if unclear
        return 0, confidence, "Unclear response, defaulting to negative"

class ColabLLaVARunner:
    """LLaVA experiment runner for Google Colab"""
    
    def __init__(self, model_name: str = "llava-hf/llava-1.5-7b-hf"):
        self.model_name = model_name
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # LLaVA prompt template (exact same as local run)
        self.prompt_template = """USER: <image>
Text: '{text}'
Metadata: {metadata}
Does the text match the image and metadata? Provide a comprehensive analysis.
ASSISTANT:"""
        
        logger.info(f"Initializing LLaVA runner with device: {self.device}")
    
    def load_model(self):
        """Load LLaVA model and processor"""
        logger.info(f"Loading LLaVA model: {self.model_name}")
        
        try:
            self.processor = LlavaProcessor.from_pretrained(self.model_name)
            self.model = LlavaForConditionalGeneration.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            logger.info("Model loaded successfully")
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
    
    def download_image(self, image_url: str) -> Image.Image:
        """Download image from URL"""
        try:
            response = requests.get(image_url, timeout=10)
            response.raise_for_status()
            image = Image.open(io.BytesIO(response.content)).convert('RGB')
            return image
        except Exception as e:
            logger.warning(f"Could not download image from {image_url}: {e}")
            # Return a dummy image
            return Image.new('RGB', (224, 224), color='gray')
    
    def create_metadata_string(self, row: pd.Series) -> str:
        """Create metadata string from row data"""
        metadata_parts = []
        
        # Add relevant metadata fields
        if pd.notna(row.get('created_utc')):
            metadata_parts.append(f"created_utc: {row['created_utc']}")
        if pd.notna(row.get('domain')):
            metadata_parts.append(f"domain: {row['domain']}")
        if pd.notna(row.get('author')):
            metadata_parts.append(f"author: {row['author']}")
        if pd.notna(row.get('subreddit')):
            metadata_parts.append(f"subreddit: {row['subreddit']}")
        
        return "; ".join(metadata_parts) if metadata_parts else "No metadata available"
    
    def process_sample(self, row: pd.Series) -> Dict[str, Any]:
        """Process a single sample"""
        try:
            # Download image
            image = self.download_image(row['image_url'])
            
            # Create metadata string
            metadata = self.create_metadata_string(row)
            
            # Create prompt
            prompt = self.prompt_template.format(
                text=row['clean_title'],
                metadata=metadata
            )
            
            # Process with LLaVA
            inputs = self.processor(
                text=prompt,
                images=image,
                return_tensors="pt"
            ).to(self.device)
            
            # Generate response
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=20,
                    do_sample=False
                )
            
            # Decode response
            generated_text = self.processor.batch_decode(
                generated_ids, 
                skip_special_tokens=True
            )[0]
            
            # Extract the assistant's response (after "ASSISTANT:")
            if "ASSISTANT:" in generated_text:
                response = generated_text.split("ASSISTANT:")[-1].strip()
            else:
                response = generated_text.strip()
            
            # Parse prediction
            parser = LLaVAAnswerParser()
            predicted_label, confidence, explanation = parser.extract_prediction(response)
            
            return {
                'id': row['id'],
                'text': row['clean_title'],
                'image_url': row['image_url'],
                'true_label': row['2_way_label'],
                'generated_text': response,
                'predicted_label': predicted_label,
                'confidence': confidence,
                'parsing_explanation': explanation,
                'metadata': metadata
            }
            
        except Exception as e:
            logger.error(f"Error processing sample {row['id']}: {e}")
            return {
                'id': row['id'],
                'text': row['clean_title'],
                'image_url': row['image_url'],
                'true_label': row['2_way_label'],
                'generated_text': f"Error: {e}",
                'predicted_label': -1,
                'confidence': 0.0,
                'parsing_explanation': f"Processing error: {e}",
                'metadata': self.create_metadata_string(row)
            }
    
    def run_experiment(self, csv_file: str, num_samples: int = 100) -> pd.DataFrame:
        """Run the complete experiment"""
        logger.info(f"Starting experiment with {num_samples} samples")
        
        # Load data
        df = pd.read_csv(csv_file)
        df = df.head(num_samples)  # Limit to requested number of samples
        
        # Load model
        self.load_model()
        
        # Process samples
        results = []
        start_time = time.time()
        
        for idx, row in df.iterrows():
            logger.info(f"Processing sample {idx+1}/{len(df)}: {row['id']}")
            
            result = self.process_sample(row)
            results.append(result)
            
            # Log progress
            if (idx + 1) % 10 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / (idx + 1)
                remaining = avg_time * (len(df) - idx - 1)
                logger.info(f"Progress: {idx+1}/{len(df)} samples. "
                          f"Avg time per sample: {avg_time:.1f}s. "
                          f"Estimated remaining: {remaining/60:.1f} minutes")
        
        # Create results DataFrame
        results_df = pd.DataFrame(results)
        
        # Calculate metrics
        valid_results = results_df[results_df['predicted_label'] != -1]
        if len(valid_results) > 0:
            accuracy = (valid_results['predicted_label'] == valid_results['true_label']).mean()
            logger.info(f"Accuracy: {accuracy:.3f} ({len(valid_results)} valid samples)")
        
        return results_df

def main():
    """Main function to run the experiment"""
    
    # Configuration
    CSV_FILE = "test_balanced_pairs_clean.csv"  # Upload this to Colab
    NUM_SAMPLES = 100
    MODEL_NAME = "llava-hf/llava-1.5-7b-hf"
    OUTPUT_FILE = "llava_results.csv"
    
    # Check if CSV file exists
    if not os.path.exists(CSV_FILE):
        logger.error(f"CSV file {CSV_FILE} not found. Please upload it to Colab.")
        return
    
    # Check GPU availability
    if torch.cuda.is_available():
        logger.info(f"GPU available: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        logger.warning("No GPU available. This will be very slow!")
    
    # Run experiment
    runner = ColabLLaVARunner(MODEL_NAME)
    results = runner.run_experiment(CSV_FILE, NUM_SAMPLES)
    
    # Save results
    results.to_csv(OUTPUT_FILE, index=False)
    logger.info(f"Results saved to {OUTPUT_FILE}")
    
    # Print summary
    print("\n" + "="*50)
    print("EXPERIMENT SUMMARY")
    print("="*50)
    print(f"Total samples: {len(results)}")
    print(f"Valid predictions: {len(results[results['predicted_label'] != -1])}")
    
    valid_results = results[results['predicted_label'] != -1]
    if len(valid_results) > 0:
        accuracy = (valid_results['predicted_label'] == valid_results['true_label']).mean()
        print(f"Accuracy: {accuracy:.3f}")
        
        # Confusion matrix
        tp = ((valid_results['predicted_label'] == 1) & (valid_results['true_label'] == 1)).sum()
        tn = ((valid_results['predicted_label'] == 0) & (valid_results['true_label'] == 0)).sum()
        fp = ((valid_results['predicted_label'] == 1) & (valid_results['true_label'] == 0)).sum()
        fn = ((valid_results['predicted_label'] == 0) & (valid_results['true_label'] == 1)).sum()
        
        print(f"True Positives: {tp}")
        print(f"True Negatives: {tn}")
        print(f"False Positives: {fp}")
        print(f"False Negatives: {fn}")
    
    print("="*50)

if __name__ == "__main__":
    main() 